# -*- coding: utf-8 -*-
"""PPO_with_RM_CALIBRATED.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s3gB6dCwTiIPmKvEFa0IrYNUKnme0iqd

Questions
"""

def getQuestions():
  questions = [
      "What is the uncertainty principle?",
      "What is the concept of entropy?",
      "What is the concept of hybridization in chemistry?",
      "Why is the sky blue?",
      "What is the role of oxidation states in redox reactions?",
      "How do we determine the structure of molecules using spectroscopy?",
      "What are the main types of galaxies?",
      "How do genes control the traits of an organism?",
      "What are the properties of noble gases?",
      "What are the mechanisms of evolution?",
      "What is the role of the immune system?",
      "How do neurons transmit signals?",
      "What is the role of symmetry in physics?",
      "What are pulsars?",
      "How do genetic mutations occur?",
      "What is the Casimir effect?",
      "What is gravitational lensing?",
      "What is the role of proteins in the body?",
      "What is the Standard Model of particle physics?",
      "What are the effects of habitat loss on ecological networks?",
      "What is the impact of human activities on the environment?",
      "How do tsunamis form and what are their effects?",
      "What are the effects of habitat destruction on species extinction?",
      "what is newton second law?",
      "How do we study and mitigate soil erosion?",
      "What is the role of RNA in gene expression?",
      "How do photosynthetic organisms capture light energy?",
      "What is the role of the atmosphere in regulating climate?",
      "What are the processes of sedimentation?",
      "What are the different types of plate boundaries?",
      "What is the role of environmental law in conservation?",
      "When it comes to waveform graphs, what does `non-convergent` mean?",
      "How are complex ions formed?",
      "How do we conserve natural habitats?",
      "How are chemical reactions balanced?",
      "How does a quantum computer work?",
      "How do we measure and predict earthquakes?",
      "How do parasites affect their hosts?",
      "How do we study and protect marine ecosystems?",
      "What is the difference between a chemical compound and a mixture?",
      "How do plants absorb nutrients?",
      "How do chemical reactions occur at the molecular level?",
      "How do we measure the distance to stars?",
      "What is the role of genetic recombination?",
      "What are fermions and bosons?",
      "What is the function of the nervous system?",
      "How does a particle accelerator work?",
      "How does the circulatory system function?",
      "What is artificial intelligence?",
      "What does a negatively charged object end up?",
      "What are asteroids and comets?",
      "How do chemical reactions affect the environment?",
      "How do astronomers detect black holes?",
      "How does the Earth's atmosphere affect astronomical observations?",
      "What is the concept of spacetime?",
      "How does thermodynamics govern energy transfer?",
      "What are neutron stars?",
      "How do enzymes catalyze biochemical reactions?",
      "What is the role of carbon in organic chemistry?",
      "What is the importance of biodiversity in ecosystems?",
      "How do plants and animals respond to stimuli?",
      "What is the difference between an exothermic and an endothermic reaction?",
      "What is the role of aerosols in the Earth's atmosphere?",
      "How do intermolecular forces affect the properties of substances?",
      "What are the effects of global warming on polar regions?",
      "What is the water cycle?",
      "How do we study and manage coastal ecosystems?",
      "How do we study and mitigate the effects of industrial pollution?",
      "What are the effects of agricultural runoff on water quality?",
      "How do earthquakes occur?",
      "Why is ice less dense than water?",
      "What is the role of ecosystem services in human well-being?",
      "How do we study and manage climate change mitigation?",
      "What are the causes and consequences of deforestation?",
      "What are the effects of environmental pollution on living organisms?",
      "what causes excessive sweating in some and hardly any drop of sweat in others?",
      "How do we study and manage sustainable development?",
      "What is the role of gauge theories in particle physics?",
      "How does the Internet work?",
      "How do plants adapt to their environment?",
      "What are the different types of star clusters?",
      "How do binary star systems work?",
      "What causes the phases of the moon?",
      "What are the stages of the cell cycle?",
      "What is the photoelectric effect?",
      "What is the heliosphere?",
      "How do cells obtain and use energy?",
      "How do animals reproduce?",
      "How do lasers work?",
      "are waves controllable?",
      "How do airplanes stay in the air?",
      "How are galaxies formed?",
      "What is the greenhouse effect?",
      "What is the role of environmental advocacy in policy change?",
      "What is the rock cycle?",
      "What is the role of carbon in the Earth's climate system?",
      "How does the human body maintain homeostasis?",
      "How do black holes form?",
      "How can tartigrades survive in outer space and vacuum?",
      "What are the effects of plastic pollution on marine life?",
      "What is the Schrödinger equation?",
      "Why do standard academic books have to be so expensive? Why are they copyright protected?",
      "How are reaction rates determined?",
      "Why do puddles dry up but not large lakes?",
      "What are the effects of microplastics on the environment?",
      "What are the different types of rocks?",
      "What is the role of environmental policies in protecting ecosystems?",
      "What is the role of the endocrine system?",
      "How do acids and bases react with each other?",
      "How does the digestive system process food?",
      "What is the role of the ozone layer?",
      "What are the major types of soil?",
      "How does Bluetooth work?",
      "What is the Higgs boson?",
      "How does soap clean?",
      "How do transition metals differ from other elements?",
      "What is a virus?",
      "What is the difference between a homogeneous and a heterogeneous catalyst?",
      "What are the differences between prokaryotic and eukaryotic cells?",
      "What is string theory?",
      "What is the difference between a nova and a supernova?",
      "What is the Goldilocks zone?",
      "What is the difference between a planet and a dwarf planet?",
      "How do we measure concentration in chemistry?",
      "How does nuclear fusion work?",
      "What is quantum entanglement?",
      "What is the role of ligands in coordination chemistry?",
      "How does gravity affect time?",
      "What is the difference between classical and quantum physics?",
      "What is the concept of stereochemistry?",
      "What is DNA and why is it important?",
      "Why are planets round?",
      "What is the concept of quantum entanglement?",
      "What is the role of environmental technology in pollution control?",
      "What is the function of the excretory system?",
      "How do we manage waste sustainably?",
      "Why are trans fats bad for you, what do they do to your body?",
      "What are the effects of chemical pollutants on human health?",
      "What is the role of the hydrosphere in the Earth's climate system?",
      "What are the principles of ecological succession?",
      "How do pollutants affect air quality?",
      "How do ecosystems function and what are their components?",
      "What are the effects of climate change on biodiversity?",
      "How do we study and manage sustainable transportation?",
      "What are the different types of cells in the human body?",
      "What is the role of biodiversity in ecosystem stability?",
      "What are the effects of ocean acidification on marine ecosystems?",
      "What is the role of solvents in chemical reactions?",
      "What is the role of chloroplasts in plant cells?",
      "Why is a password with both numbers and letters stronger than one with only letters? Attackers will include numbers in their brute force attempts anyway, so how does it make a difference?",
      "Why isn’t Switzerland a desert?",
      "If air molecules are acting like masses on a spring in sound waves, how are different frequencies possible?",
      "How do so many cave dwelling species evolve similar exotic traits like losing eyes, clear skin, etc?",
      "How come if we jump inside a train we land on the same spot but if jumped on top of it we land at a different one?",
      "Is there a minimum gravity required to hold a breathable atmosphere?",
      "What, if any, are mechanisms human body uses to prevent/resolve unwanted blood clots?",
      "Why don’t plants get cancer?",
      "Is there another reservoir for the herpes simplex viruses than humans?",
      "What Factors lead to Polygyny in Animals, and what Factors lead to Monogamy?",
      "Why can the speed of light in a medium be faster than c?",
      "Why are photons the only force carriers that are “visible”?",
      "How do we identify gene variants?",
      "What was in our area of space before our sun existed?",
      "What's with the apocalyptic floods we're seeing more and more?",
      "Is Betelgeuse (almost) spherical like other stars or is it more like a blob?",
      "If birds evolved from dinosaurs, what natural selection feature of the birds made them evolve to a much smaller size compared to dinos?",
      "How do amps pick up radio signals?",
      "During pregnancy what provides the pressure to maintain the amniotic sac and force the belly to stretch and grow?",
      "How do photons represent electromagnetic fields over large distances with many particles?",
      "Is grey fox really a fox?",
      "When the 1st logarithmic scales for slide rules were created, how did they make *precise* lengths and divisions? Also - is there a geometric construction that precisely gives logarithmic scales?",
      "Does total fertility rate calculation account for time?",
      "If birds are descendents of reptiles, when and how did they become warm blooded?",
      "If the laws of physics would work the same if time flowed backwards, how does entropy play into that?",
      "When editing DNA/genes, how does the human body know to replicate the edited version vs. the original?",
      "Voyager Spacecraft just lucky?",
      "Can animals recognise depictions of themselves through art?",
      "Do we know what the cores of stars look like?",
      "Was any of the ancestors of the penguin able to fly?",
      "How does the anatomy of the eyes prevent water from entering the ocular cavity? Is there some biological watertight seal? Why doesn’t water get in when say diving or when rinsing one’s eyes in the faucet?",
      "How can your immune system allergically react to a substance that hasn't entered your body/bloodstream?",
      "Is it true CO2 emission sequestering is useless and we should only care about reduction or avoidance?",
      "Why did it take so long for Apollo 11 to reach the moon?",
      "Do fish have saliva?",
      "Are the images from the James Webb telescope the true images captured or is there some rendering or interpretation added in?",
      "How does testosterone levels during puberty affect the growth and development of the penis?",
      "In 250 million years, how much longer will a year be?",
      "Will the moon ever be destroyed?",
      "Windmills, why aren’t they always “on”?",
      "Is it possible to have an ice age  and a supercontinent at the same time?",
      "Why does Delta IV set itself on fire when other rockets don't?",
      "If we say time is the 4th dimension, why don't we just attribute dimensions to other things, like `the 5th dimension is charge`?",
      "Do any marine mammals have underwater sense of smell?",
      "Why does spinning your body in a circle result in dizziness, nausea, and vomiting?",
      "How does the lung expel germs?",
      "Does the Amazon rainforest have an effect on the Sahara desert?",
      "How do we know what is normal for climate fluctuations throughout Earth's history if we only have limited data points rather than day to day statistics of pre-human weather?",
      "The moon has many craters visible to the naked eye, what would the impact event that created the largest of them have looked like to the naked eye from earth?",
      "Now that one of the three major strains of Influenza is effectively extinct, have the other two increased in prevalence to compensate?",
      "Can dinosaur bones that are created in sandstone have a thin layer of sandstone covering the bones that make them not look like bones?",
      "What’s happening in your body that causes the sensation of fatigue? Not the macro causes like poor sleep, etc. Decrease or increase of a particular hormone or something along those lines?",
      "Why did the Laurentide Ice Sheet form and receed like it did?",
      "How are the varying kinetic energies and momentums from different reference frames balanced when dealing with relativistic speeds?",
      "How many stars in the sky don't exist?",
      "What does an unborn baby have in it's lungs?",
      "Why are there dunes?",
      "In placental mammals, does the placenta come from the embryo or the mother?",
      "Why is there a visible cone of a tornado when it’s made of rotating air?",
      "do we have more or less trees than we did 30 years ago?",
      "Does being exposed to a disease during a period of immunity prolong your resistance?",
      "Why five fingers? Why not 3, 7, or 9?",
      "What causes arctic animals to shift between summer and winter coats?",
      "Are there invasive species from the Americas in Africa/Asia/Europe?",
      "Mt Rainier lasted erupted in 1450 CE and has a pointy peak. How long does it take for a volcano to turn into a peak?",
      "How long would it take for the land under antarctica to fully resurface if all the ice above it dissappeared this instant?",
      "How long does an `evolution` of a rhinovirus last before going extinct?",
      "Is energy actually conserved?",
      "How can gene edits prevent viral infection?",
      "It is widely said that age of the universe is around 14 billion years. According to whom?",
      "Why did NASA's Orion capsule change colour after reentry?",
      "Why were there massive spikes in exoplanets discovered in 2014 and 2016?",
      "How does both nuclear fusion and nuclear fission release energy?",
      "Axiomatic definition of classes of L functions?",
      "How exactly does the Pauli Exclusion Principle play a role in contact forces vs electrostatic repulsion?",
      "Why is one side of the Mississippi River flat, and the other side hilly?",
      "What dictates which field is excited in QFT?",
      "If moving at near (90+%) light speed, can a person travel 'n' light-years and experience LESS than 'n' years, or is their minimum perceived travel time equal to the number of light years in the journey?",
      "What were all the small explosion tests in Oppenheimer?",
      "How is the Internet speed at the ISS over 1GB/sec?",
      "Which river on earth is currently changing its course the fastest/slowest? How can we tell?",
      "How do birds know which way is south?",
      "How do researchers quantify depression,anxiety,bipolar etc in mice models?",
      "Can crows from different regions communicate?",
      "How can eyes heal themselves when they have immune privilege?",
      "How fast do macrophages move around?",
      "Why do some insect fossils still have colors ?",
      "How does lactose enter lac operon?",
      "What is the deal with co-occurring allergies that don't make sense in terms of clades?",
      "What's the layman definitions for the SI units?",
      "Why can't cats with FIV (Feline Immunodeficiency Virus) be given medication for HIV (Human Immunodeficiency Virus)?",
      "How do researchers give lab rats cancer?",
      "Why do banana peels connected to the banana turn black in the fridge even if not ripe?",
      "What's the limit to the placebo effect?",
      "Do octopodes change color beyond the visible spectrum of light?",
      "Is the flavor of a strawberry concentrated in certain parts of the fruit, or is it distributed evenly throughout?",
      "Why is body/facial hair such a strongly sex-linked trait in humans? Is there any potential evolutionary reason for it being correlated with testosterone and present largely only in males?",
      "if you put a scale underwater with the face pointing up, would it show the weight of the water above it?",
      "How is it possible to hear sound under water?",
      "How do ecosystems function?",
      "What are the fundamental forces of nature?",
      "How do electric and magnetic fields interact?",
      "What is the process of mitosis and meiosis?",
      "What is the significance of the Hubble Space Telescope?",
      "What is evolution?",
      "Why do leaves fall in autumn?",
      "Why do we have seasons?",
      "How do planetary rings form?",
      "How do cameras work?",
      "Why does salt melt ice?",
      "What are allotropes and how do they differ?",
      "How do hormones regulate bodily functions?",
      "What is a supernova?",
      "What causes thunder?",
      "What is the role of ribosomes in protein synthesis?",
      "What causes earthquakes?",
      "How TVs switched from b&w to color?",
      "What are the different types of sedimentary rocks?",
      "What is the periodic table and how is it organized?",
      "What is the role of environmental monitoring in protecting ecosystems?",
      "What are the processes of tectonic deformation?",
      "What are the effects of urbanization on natural habitats?",
      "How does light behave as both a wave and a particle?",
      "How do we study and manage air quality?",
      "What are the effects of land use change on ecosystems?",
      "What is the role of conservation biology in protecting species?",
      "How do we study and protect wetlands?",
      "why are heartbeat, pulse, breathing like a wave? Why do wave exists in the universe?",
      "What is the role of neurotransmitters in the brain?",
      "What are the layers of the Earth?",
      "How do plants reproduce sexually and asexually?",
      "What is the role of geochemical cycles in the Earth's system?",
      "How are mountains formed?",
  ]
  return questions

"""code"""

# -*- coding: utf-8 -*-
"""
PPO Training with DeepSeek-based Tiered Reward Model for Scientific Explanations
Author: SciGuru Team
Description: This implementation trains a language model to provide better scientific explanations
using PPO with a sophisticated reward model based on pedagogical quality metrics.
"""

# ========================================
# 1. Setup and Installation
# ========================================

# Install required packages
!pip install torch transformers peft accelerate bitsandbytes -q
!pip install tqdm psutil requests -q

# Create directories for saving our models
!mkdir -p ppo_adapter
!mkdir -p offload_folder
!mkdir -p reward_cache

# ========================================
# 2. Imports and Configuration
# ========================================

import os
import torch
import torch.nn.functional as F
import numpy as np
import gc
import json
import time
import requests
from datetime import datetime
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict
import random
import shutil
import glob

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
)
from peft import (
    LoraConfig,
    PeftModel,
    get_peft_model,
    prepare_model_for_kbit_training
)
from tqdm.notebook import tqdm
import logging
import psutil
import matplotlib.pyplot as plt
from google.colab import auth
import warnings
warnings.filterwarnings('ignore')

# Configure logging for better debugging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Enable memory efficient operations
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# Check GPU availability
print(f"GPU available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

# ========================================
# 3. Configuration and API Setup
# ========================================

from getpass import getpass

def setup_tokens():
    """Set up Hugging Face and DeepSeek API tokens"""
    # HuggingFace token
    if os.environ.get("HF_READ_TOKEN") is None:
        hf_token = getpass("Enter your Hugging Face token: ")
        os.environ["HF_READ_TOKEN"] = hf_token

    # DeepSeek API token
    if os.environ.get("DEEPSEEK_API_KEY") is None:
        ds_token = getpass("Enter your DeepSeek API key: ")
        os.environ["DEEPSEEK_API_KEY"] = ds_token

    return os.environ["HF_READ_TOKEN"], os.environ["DEEPSEEK_API_KEY"]

# Get our API tokens
AUTH_TOKEN, DEEPSEEK_API_KEY = setup_tokens()

# Model configuration
BASE_MODEL_NAME = "meta-llama/Llama-3.2-3B-Instruct"
ADAPTER_PATH = "ppo_adapter"
OFFLOAD_FOLDER = "offload_folder"
REWARD_CACHE_PATH = "reward_cache/cache.json"

# PPO Hyperparameters - tuned for scientific explanations
LEARNING_RATE = 5e-6  # Reduced from 1e-5 for stability
CLIP_EPSILON = 0.1    # Tighter clipping (was 0.2)
KL_PENALTY = 0.15      # Increased from 0.1 to prevent drift
VALUE_COEF = 1.0      # Increased from 0.5 for better value learning
ENTROPY_COEF = 0.02   # Increased for more exploration
NUM_EPOCHS = 4        # More epochs per iteration
BATCH_SIZE = 4        # Larger batches if memory allows
GRADIENT_ACCUMULATION_STEPS = 8  # Effective batch = 32
MAX_SEQ_LENGTH = 512  # Longer for detailed explanations
MAX_GRAD_NORM = 1.0
GAMMA = 0.99
GAE_LAMBDA = 0.95

# LoRA Configuration - memory efficient fine-tuning
lora_config = LoraConfig(
    r=4,  # Low rank for efficiency
    lora_alpha=8,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj", "o_proj"],  # Target attention layers
)

# DeepSeek API configuration
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"
DEEPSEEK_MODEL = "deepseek-chat"
MAX_RETRIES = 3
RETRY_DELAY = 2

# ========================================
# 4. Reward Model Components with CALIBRATION
# ========================================

@dataclass
class TieredWeights:
    """Configuration for our tiered reward system weights"""
    # Tier weights - how important each tier is
    tier1_weight: float = 0.6  # Core pedagogical clarity
    tier2_weight: float = 0.3  # Cognitive tools
    tier3_weight: float = 0.1  # Secondary metrics

    # Individual metric weights within tiers
    # Tier 1 metrics (must sum to 1.0)
    articulation: float = 0.25
    internal_coherence: float = 0.25
    completeness: float = 0.20
    audience_awareness: float = 0.15
    jargon: float = 0.15

    # Tier 2 metrics (must sum to 1.0)
    analogy: float = 0.40
    explanation_type: float = 0.35
    connection_everyday: float = 0.25

    # Tier 3 metrics (must sum to 1.0)
    correctness: float = 0.40
    perceived_truth: float = 0.35
    content_units: float = 0.25

class DeepSeekRewardModel:
    """
    Sophisticated reward model using DeepSeek API to evaluate scientific explanations
    based on multiple pedagogical quality metrics.

    CALIBRATED VERSION: Creates more room for improvement by adjusting scoring
    """

    def __init__(self, api_key: str, cache_path: str = REWARD_CACHE_PATH):
        self.api_key = api_key
        self.cache_path = cache_path
        self.weights = TieredWeights()
        self.cache = self._load_cache()

        # Headers for API requests
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        # CALIBRATION PARAMETERS
        self.calibration_enabled = True
        self.baseline_shift = -3.5  # Shift all scores down by 3.5 points
        self.scale_factor = 1.8     # Amplify differences between scores
        self.power_transform = 1.5  # Non-linear transformation exponent

        # Individual metric calibration offsets (metric-specific adjustments)
        self.metric_offsets = {
            'articulation': -4.0,        # Harder to get high articulation
            'internal_coherence': -3.5,
            'completeness': -4.5,        # Much harder for completeness
            'audience_awareness': -3.0,
            'jargon': -2.5,             # Less harsh on jargon
            'analogy': -5.0,            # Very hard to get credit for analogies
            'explanation_type': -3.5,
            'connection_everyday': -4.0,
            'correctness': -2.0,        # Less harsh on correctness
            'perceived_truth': -2.5,
            'content_units': -3.0
        }

    def _load_cache(self) -> Dict:
        """Load cached evaluations to avoid redundant API calls"""
        if os.path.exists(self.cache_path):
            with open(self.cache_path, 'r') as f:
                return json.load(f)
        return {}

    def _save_cache(self):
        """Save evaluation cache"""
        os.makedirs(os.path.dirname(self.cache_path), exist_ok=True)
        with open(self.cache_path, 'w') as f:
            json.dump(self.cache, f)

    def _calibrate_score(self, raw_score: float, metric_name: str = None) -> float:
        """
        Apply calibration to make scoring more challenging and create room for improvement

        Args:
            raw_score: Original score from DeepSeek (0-10)
            metric_name: Name of the metric for specific calibration

        Returns:
            Calibrated score (0-10)
        """
        if not self.calibration_enabled:
            return raw_score

        # Apply metric-specific offset if available
        if metric_name and metric_name in self.metric_offsets:
            calibrated = raw_score + self.metric_offsets[metric_name]
        else:
            calibrated = raw_score + self.baseline_shift

        # Ensure we stay in valid range before scaling
        calibrated = max(0, calibrated)

        # Apply scaling to amplify differences
        calibrated = calibrated * self.scale_factor

        # Apply non-linear transformation to make high scores much harder
        # This creates a curve where improvements get progressively harder
        if calibrated > 0:
            calibrated = 10 * (calibrated / 10) ** self.power_transform

        # Final clamp to valid range
        calibrated = max(0, min(10, calibrated))

        # Add small noise to prevent identical scores
        calibrated += random.uniform(-0.1, 0.1)
        calibrated = max(0, min(10, calibrated))

        return calibrated

    def _call_deepseek(self, prompt: str, metric_name: str = None) -> Optional[float]:
        """
        Make API call to DeepSeek with retry logic
        Returns a CALIBRATED score between 0-10
        """
        for attempt in range(MAX_RETRIES):
            try:
                payload = {
                    "model": DEEPSEEK_MODEL,
                    "messages": [
                        {"role": "system", "content": "You are an expert evaluator of scientific explanations. Provide only numerical scores as requested."},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.1,  # Low temperature for consistent scoring
                    "max_tokens": 10
                }

                response = requests.post(
                    DEEPSEEK_API_URL,
                    headers=self.headers,
                    json=payload,
                    timeout=30
                )

                if response.status_code == 200:
                    result = response.json()
                    score_text = result['choices'][0]['message']['content'].strip()

                    # Extract numerical score
                    try:
                        raw_score = float(score_text)
                        raw_score = max(0, min(10, raw_score))  # Clamp to 0-10

                        # Apply calibration
                        calibrated_score = self._calibrate_score(raw_score, metric_name)

                        logger.debug(f"Metric: {metric_name}, Raw: {raw_score:.2f}, Calibrated: {calibrated_score:.2f}")

                        return calibrated_score

                    except ValueError:
                        logger.warning(f"Could not parse score: {score_text}")
                        return self._calibrate_score(5.0, metric_name)  # Calibrated default

                elif response.status_code == 429:  # Rate limit
                    logger.warning(f"Rate limit hit, waiting {RETRY_DELAY * (attempt + 1)}s")
                    time.sleep(RETRY_DELAY * (attempt + 1))
                else:
                    logger.error(f"API error: {response.status_code} - {response.text}")

            except Exception as e:
                logger.error(f"Error calling DeepSeek API: {str(e)}")

            if attempt < MAX_RETRIES - 1:
                time.sleep(RETRY_DELAY)

        return self._calibrate_score(5.0, metric_name)  # Calibrated default if all retries fail

    # Update all evaluation methods to include question parameter
    def evaluate_articulation(self, question: str, response: str) -> float:
        """Evaluate how well articulated the explanation is"""
        prompt = f"""
Evaluate the following scientific explanation for articulation quality.

Question asked: {question}
Explanation given: {response}

Evaluation steps:
1. Determine whether the explanation is well articulated FOR THIS SPECIFIC QUESTION
2. Consider clarity of expression, logical flow, and coherent sentence structure
3. A score of 10 means perfectly articulated, 0 means terribly articulated
4. Ignore factual correctness - focus only on how well ideas are expressed

Provide only a number between 0 and 10:"""

        return self._call_deepseek(prompt, 'articulation')

    def evaluate_internal_coherence(self, question: str, response: str) -> float:
        """Check if parts of the explanation fit together logically"""
        prompt = f"""
Evaluate the internal coherence of this scientific explanation.

Question asked: {question}
Explanation given: {response}

Evaluation steps:
1. Check if all parts of the explanation connect logically to answer the question
2. Look for contradictions or disconnected ideas
3. Score 10 if perfectly coherent, 0 if completely disjointed
4. Ignore accuracy - focus only on logical consistency

Provide only a number between 0 and 10:"""

        return self._call_deepseek(prompt, 'internal_coherence')

    def evaluate_completeness(self, question: str, response: str) -> float:
        """Check for gaps in the explanation"""
        prompt = f"""
Evaluate the completeness of this explanation.

Definition: A gap exists when the explanation suggests A causes B but doesn't explain HOW.

Question: {question}
Answer: {response}

Evaluation steps:
1. Identify any logical gaps in the causal chain
2. Score 10 if no gaps exist, 0 if many gaps exist
3. Focus on explanatory completeness, not correctness

Provide only a number between 0 and 10:"""

        return self._call_deepseek(prompt, 'completeness')

    def evaluate_audience_awareness(self, question: str, response: str) -> float:
        """Check if appropriate for general audience"""
        prompt = f"""
Determine whether the explanation is appropriate for a general audience with basic scientific knowledge (high school level).

Question asked: {question}
Explanation given: {response}

Evaluation steps:
1. Check if technical terms are explained when introduced
2. Assess if concepts build from simple to complex
3. Score between 0 and 10: 10 means it explains well to a non-expert; 0 means it uses overly technical or inaccessible language.
4. Ignore whether the content is factually correct—focus only on clarity and appropriateness of explanation level.

Provide only a number between 0 and 10:"""

        return self._call_deepseek(prompt, 'audience_awareness')

    def evaluate_jargon(self, question: str, response: str) -> float:
        """Evaluate use of technical language (less jargon = higher score)"""
        prompt = f"""
Evaluate the use of jargon in this scientific explanation.

Question asked: {question}
Explanation given: {response}

Evaluation steps:
1. Count unexplained terms, general scientific concepts and technical terms
2. Check if specialized vocabulary is necessary and well-explained
3. Score between 0 and 10: 10 means minimal jargon; 0 means excessive unexplained jargon
4. Reward clear, simple language

Provide only a number between 0 and 10:"""

        return self._call_deepseek(prompt, 'jargon')

    def evaluate_analogy(self, question: str, response: str) -> float:
        """Check for use of analogies"""
        prompt = f"""
Evaluate the use of analogies in this explanation.

Definition: Analogies map between familiar (source) and novel (target) situations.

Question asked: {question}
Explanation given: {response}

Evaluation steps:
1. Identify any analogies used
2. Score 10 if effective analogies present, 0 if none
3. Consider quality and appropriateness of analogies
4. Focus on pedagogical value, not accuracy

Provide only a number between 0 and 10:"""

        return self._call_deepseek(prompt, 'analogy')

    def evaluate_explanation_type(self, question: str, response: str) -> float:
        """Classify and score explanation sophistication"""
        prompt = f"""
Given the below 5 numbered categories of explanation, assign a score matching the most advanced explanation type present in the answer.

Types (with scores):
- Absent (0): No explanation
- Definition (2.5): A short definition of a certain entity is present, without further explanation.
- Elucidating (5): A definition with at least one example.
- Transformative (7.5): Any explanation whose starting point is what the audience might think, that points to problems with the existing conceptions.
- Quasiscientific (10): An explanation that creates an image in the mind, often by using an analogy.

Question asked: {question}
Explanation given: {response}

If an answer contains multiple types of explanations, assign the score based on the best explanation type in the answer.

Provide only the numerical score:"""

        return self._call_deepseek(prompt, 'explanation_type')

    def evaluate_everyday_connection(self, question: str, response: str) -> float:
        """Check for connections to everyday life"""
        prompt = f"""
Check if the explanation contains an explicit connection to common knowledge, a previous event, or a news story that was not already embedded in the question.

Question asked: {question}
Explanation given: {response}

Score 10 if clear everyday connections exist, 0 if none.
Focus on relatability and content, not accuracy or structure.

Provide only a number between 0 and 10:"""

        return self._call_deepseek(prompt, 'connection_everyday')

    def evaluate_correctness(self, question: str, response: str) -> float:
        """Evaluate factual accuracy"""
        prompt = f"""
Evaluate the scientific correctness of this answer.

Question: {question}
Answer: {response}

Score 10 for completely correct, 0 for completely wrong.
Consider current scientific consensus.

Provide only a number between 0 and 10:"""

        return self._call_deepseek(prompt, 'correctness')

    def evaluate_perceived_truth(self, question: str, response: str) -> float:
        """Evaluate believability"""
        prompt = f"""
You are an average person. Read the question and answer, and decide if you believe the answer or not.

Question: {question}
Answer: {response}

Score 10 if completely believable, 0 if obviously false.
Consider plausibility and presentation, not technical accuracy.

Provide only a number between 0 and 10:"""

        return self._call_deepseek(prompt, 'perceived_truth')

    def evaluate_content_units(self, question: str, response: str) -> float:
        """Count standalone facts"""
        prompt = f"""
Count the number of standalone facts in this explanation (that does not repeat).
A standalone fact is information that makes sense independently.

Question asked: {question}
Explanation given: {response}

Scoring:
- 0-2 facts: score 2
- 3-4 facts: score 4
- 5-6 facts: score 6
- 7-8 facts: score 8
- 9+ facts: score 10

Provide only the score:"""

        return self._call_deepseek(prompt, 'content_units')

    def calculate_reward(self, question: str, response: str) -> Tuple[float, Dict]:
        """
        Calculate the final reward using our tiered system with CALIBRATION
        Returns: (final_reward, detailed_scores)
        """
        # Create cache key
        cache_key = f"{question[:50]}_{response[:50]}"

        # Check cache first
        if cache_key in self.cache:
            logger.info("Using cached reward evaluation")
            return self.cache[cache_key]['final_reward'], self.cache[cache_key]

        logger.info("Evaluating response with DeepSeek (CALIBRATED)...")

        # Evaluate all metrics (now with calibration and question context)
        scores = {
            # Tier 1 - Core Pedagogical Clarity
            'articulation': self.evaluate_articulation(question, response),
            'internal_coherence': self.evaluate_internal_coherence(question, response),
            'completeness': self.evaluate_completeness(question, response),
            'audience_awareness': self.evaluate_audience_awareness(question, response),
            'jargon': self.evaluate_jargon(question, response),

            # Tier 2 - Cognitive Tools
            'analogy': self.evaluate_analogy(question, response),
            'explanation_type': self.evaluate_explanation_type(question, response),
            'connection_everyday': self.evaluate_everyday_connection(question, response),

            # Tier 3 - Secondary Metrics
            'correctness': self.evaluate_correctness(question, response),
            'perceived_truth': self.evaluate_perceived_truth(question, response),
            'content_units': self.evaluate_content_units(question, response)
        }

        # Calculate tier scores (already using calibrated individual scores)
        tier1_score = (
            scores['articulation'] * self.weights.articulation +
            scores['internal_coherence'] * self.weights.internal_coherence +
            scores['completeness'] * self.weights.completeness +
            scores['audience_awareness'] * self.weights.audience_awareness +
            scores['jargon'] * self.weights.jargon
        )

        tier2_score = (
            scores['analogy'] * self.weights.analogy +
            scores['explanation_type'] * self.weights.explanation_type +
            scores['connection_everyday'] * self.weights.connection_everyday
        )

        tier3_score = (
            scores['correctness'] * self.weights.correctness +
            scores['perceived_truth'] * self.weights.perceived_truth +
            scores['content_units'] * self.weights.content_units
        )

        # Calculate final weighted reward
        final_reward = (
            tier1_score * self.weights.tier1_weight +
            tier2_score * self.weights.tier2_weight +
            tier3_score * self.weights.tier3_weight
        )

        # Log calibration effect
        logger.info(f"CALIBRATED Final reward: {final_reward:.2f} (T1: {tier1_score:.2f}, T2: {tier2_score:.2f}, T3: {tier3_score:.2f})")
        logger.info(f"Individual calibrated scores: {scores}")

        # Prepare detailed results
        detailed_scores = {
            'final_reward': final_reward,
            'tier_scores': {
                'tier1': tier1_score,
                'tier2': tier2_score,
                'tier3': tier3_score
            },
            'individual_scores': scores,
            'question': question,
            'response': response,
            'calibration_applied': self.calibration_enabled
        }

        # Cache the result
        self.cache[cache_key] = detailed_scores
        self._save_cache()

        return final_reward, detailed_scores

    def set_calibration(self, enabled: bool = True, baseline_shift: float = None,
                       scale_factor: float = None, power_transform: float = None):
        """
        Adjust calibration parameters dynamically

        Args:
            enabled: Whether to apply calibration
            baseline_shift: How much to shift scores down
            scale_factor: How much to amplify differences
            power_transform: Non-linear transformation exponent
        """
        self.calibration_enabled = enabled

        if baseline_shift is not None:
            self.baseline_shift = baseline_shift
        if scale_factor is not None:
            self.scale_factor = scale_factor
        if power_transform is not None:
            self.power_transform = power_transform

        logger.info(f"Calibration updated: enabled={enabled}, shift={self.baseline_shift}, "
                   f"scale={self.scale_factor}, power={self.power_transform}")

    def get_calibration_stats(self) -> Dict:
        """Get statistics about calibration effects"""
        return {
            "enabled": self.calibration_enabled,
            "baseline_shift": self.baseline_shift,
            "scale_factor": self.scale_factor,
            "power_transform": self.power_transform,
            "metric_offsets": self.metric_offsets
        }

# ========================================
# 5. Utility Functions
# ========================================

def log_memory_usage(stage=""):
    """Track memory usage throughout training"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**2
        reserved = torch.cuda.memory_reserved() / 1024**2
        logger.info(f"{stage} - GPU Memory: {allocated:.2f}MB allocated, {reserved:.2f}MB reserved")

    process = psutil.Process()
    ram = process.memory_info().rss / 1024**2
    logger.info(f"{stage} - RAM Usage: {ram:.2f}MB")

def format_prompt(question):
    """Format questions for our model"""
    # Using a prompt that encourages clear, educational explanations
    prompt = f"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are an expert science educator. Explain scientific concepts clearly and simply, using analogies and everyday examples when possible. Make your explanations accessible to someone with basic high school science knowledge.<|eot_id|>
<|start_header_id|>user<|end_header_id|>
{question}<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
"""
    return prompt

# ========================================
# 6. Model Loading Functions
# ========================================

def load_models():
    """
    Load policy model, reference model, and tokenizer with memory optimizations
    Handles both fresh starts and continuing from checkpoints
    """
    logger.info("Starting model loading process...")
    log_memory_usage("Before loading")

    # Load tokenizer
    logger.info("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, token=AUTH_TOKEN)

    # Set up tokenizer properly
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # Quantization config for memory efficiency
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4"
    )

    # Load policy model
    logger.info("Loading policy model with 4-bit quantization...")
    policy_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto",
        token=AUTH_TOKEN,
        torch_dtype=torch.float16,
        offload_folder=OFFLOAD_FOLDER
    )

    # Prepare for training
    policy_model = prepare_model_for_kbit_training(policy_model)

    # Check if we have existing LoRA adapters
    if os.path.exists(ADAPTER_PATH) and os.path.exists(f"{ADAPTER_PATH}/adapter_config.json"):
        logger.info(f"Loading existing LoRA adapters from {ADAPTER_PATH}")
        policy_model = PeftModel.from_pretrained(policy_model, ADAPTER_PATH)
    else:
        logger.info("Creating new LoRA adapters")
        policy_model = get_peft_model(policy_model, lora_config)

    log_memory_usage("After policy model")

    # Load reference model (frozen, no LoRA)
    logger.info("Loading reference model...")
    reference_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto",
        token=AUTH_TOKEN,
        torch_dtype=torch.float16,
        offload_folder=OFFLOAD_FOLDER
    )

    # Freeze reference model
    for param in reference_model.parameters():
        param.requires_grad = False
    reference_model.eval()

    # Create value head for PPO
    value_head = torch.nn.Linear(
        policy_model.config.hidden_size, 1
    ).to(policy_model.device)

    # Load value head if it exists
    value_head_path = f"{ADAPTER_PATH}/value_head.pt"
    if os.path.exists(value_head_path):
        logger.info(f"Loading existing value head from {value_head_path}")
        value_head.load_state_dict(torch.load(value_head_path))
    else:
        logger.info("Initializing new value head")
        torch.nn.init.normal_(value_head.weight, mean=0.0, std=0.02)
        torch.nn.init.zeros_(value_head.bias)

    # Clear memory
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

    log_memory_usage("After all models loaded")

    return policy_model, reference_model, value_head, tokenizer

# ========================================
# 6-b. Fail-Safe mechanism for run crashing (learning from mistakes)
# ========================================
def get_resume_iteration(adapter_path):
    """Check for existing checkpoints and determine where to resume"""
    checkpoint_dirs = glob.glob(f"{adapter_path}/checkpoint-*")
    if not checkpoint_dirs:
        return 0

    # Extract iteration numbers
    iterations = []
    for cp_dir in checkpoint_dirs:
        try:
            iter_num = int(cp_dir.split('-')[-1])
            iterations.append(iter_num)
        except:
            continue

    return max(iterations) if iterations else 0

# ========================================
# 7. Rollout Generation with DeepSeek Rewards
# ========================================

def generate_rollouts_with_rewards(
    policy_model,
    reference_model,
    value_head,
    tokenizer,
    prompts,
    reward_model,
    iteration,
    max_length=MAX_SEQ_LENGTH
):
    """
    Generate rollouts and calculate rewards using our DeepSeek reward model
    This is where we integrate the sophisticated evaluation
    """

    # Check for cached rollouts (fail-safe)
    rollout_cache_file = f"{ADAPTER_PATH}/rollouts_iter_{iteration}.pkl"
    if os.path.exists(rollout_cache_file):
        logger.info(f"Loading cached rollouts from {rollout_cache_file}")
        import pickle
        with open(rollout_cache_file, 'rb') as f:
            return pickle.load(f)

    policy_model.eval()
    reference_model.eval()
    value_head.eval()

    logger.info(f"Generating rollouts for {len(prompts)} prompts...")

    all_samples = {
        "prompt_ids": [],
        "response_ids": [],
        "logprobs": [],
        "ref_logprobs": [],
        "values": [],
        "rewards": [],
        "generated_texts": [],
        "questions": [],
        "detailed_rewards": []
    }

    with torch.no_grad():
       for i, prompt in enumerate(tqdm(prompts, desc="Generating rollouts")):
           logger.info(f"\nProcessing prompt {i+1}/{len(prompts)}: {prompt[:50]}...")

           # Format and tokenize prompt
           formatted_prompt = format_prompt(prompt)
           prompt_inputs = tokenizer(
               formatted_prompt,
               return_tensors="pt",
               truncation=True,
               max_length=MAX_SEQ_LENGTH // 2
           ).to(policy_model.device)

           prompt_length = prompt_inputs.input_ids.shape[1]

           # Generate response
           generation_outputs = policy_model.generate(
               prompt_inputs.input_ids,
               attention_mask=prompt_inputs.attention_mask,
               max_new_tokens=min(max_length - prompt_length, 256),
               do_sample=True,
               temperature=0.8,  # Slightly higher for more creative explanations
               top_p=0.9,
               pad_token_id=tokenizer.pad_token_id,
               return_dict_in_generate=True,
               output_scores=True,
           )

           full_sequence = generation_outputs.sequences[0]
           response_ids = full_sequence[prompt_length:]

           if len(response_ids) <= 2:
               logger.warning(f"Skipping - response too short")
               continue

           # Decode the generated text
           generated_text = tokenizer.decode(response_ids, skip_special_tokens=True)
           logger.info(f"Generated response: {generated_text[:100]}...")

           # Calculate reward using DeepSeek
           final_reward, detailed_scores = reward_model.calculate_reward(prompt, generated_text)

           # Process tokens for PPO (similar to original but with real rewards)
           response_length = len(response_ids) - 1
           if response_length <= 0:
               continue

           # Process in chunks to avoid OOM
           chunk_size = 32
           policy_logprobs_list = []
           ref_logprobs_list = []
           values_list = []

           for chunk_start in range(0, response_length, chunk_size):
               chunk_end = min(chunk_start + chunk_size, response_length)

               # Create contexts for this chunk
               batch_contexts = []
               target_tokens = []

               for t in range(chunk_start, chunk_end):
                   context = full_sequence[:prompt_length + t + 1]
                   target = response_ids[t + 1]
                   batch_contexts.append(context)
                   target_tokens.append(target)

               # Pad and process
               padded_batch = tokenizer.pad(
                   {"input_ids": batch_contexts},
                   padding=True,
                   return_tensors="pt"
               ).to(policy_model.device)

               batch_input_ids = padded_batch["input_ids"]
               batch_attention_masks = padded_batch["attention_mask"]
               target_tokens = torch.tensor(target_tokens, device=policy_model.device)

               # Get policy logprobs and values
               policy_outputs = policy_model(
                   input_ids=batch_input_ids,
                   attention_mask=batch_attention_masks,
                   output_hidden_states=True
               )

               policy_logits = policy_outputs.logits[:, -1, :]
               policy_log_probs = torch.log_softmax(policy_logits, dim=-1)
               policy_logprobs = policy_log_probs.gather(1, target_tokens.unsqueeze(1)).squeeze(1)
               policy_logprobs_list.extend(policy_logprobs.cpu().tolist())

               # Get values from value head
               hidden_states = policy_outputs.hidden_states[-1][:, -1, :]
               values = value_head(hidden_states).squeeze(-1)
               values_list.extend(values.cpu().tolist())

               # Get reference logprobs
               ref_outputs = reference_model(
                   input_ids=batch_input_ids,
                   attention_mask=batch_attention_masks
               )

               ref_logits = ref_outputs.logits[:, -1, :]
               ref_log_probs = torch.log_softmax(ref_logits, dim=-1)
               ref_logprobs = ref_log_probs.gather(1, target_tokens.unsqueeze(1)).squeeze(1)
               ref_logprobs_list.extend(ref_logprobs.cpu().tolist())

               # Clean up
               del policy_outputs, ref_outputs, hidden_states

           # Distribute reward across tokens (can be improved with reward shaping)
           # For now, we'll give higher reward to later tokens to encourage complete explanations
           token_rewards = []
           for t in range(len(policy_logprobs_list)):
               # Linear increase from 0.5x to 1.5x of the final reward
               position_weight = 0.5 + (t / len(policy_logprobs_list))
               token_reward = (final_reward / 10.0) * position_weight  # Normalize to 0-1
               token_rewards.append(token_reward)

           # Store all data
           all_samples["prompt_ids"].append(prompt_inputs.input_ids[0].cpu())
           all_samples["response_ids"].append(response_ids.cpu())
           all_samples["logprobs"].append(policy_logprobs_list)
           all_samples["ref_logprobs"].append(ref_logprobs_list)
           all_samples["values"].append(values_list)
           all_samples["rewards"].append(token_rewards)
           all_samples["generated_texts"].append(generated_text)
           all_samples["questions"].append(prompt)
           all_samples["detailed_rewards"].append(detailed_scores)

           # Log reward breakdown for monitoring
           logger.info(f"Reward breakdown - T1: {detailed_scores['tier_scores']['tier1']:.2f}, "
                     f"T2: {detailed_scores['tier_scores']['tier2']:.2f}, "
                     f"T3: {detailed_scores['tier_scores']['tier3']:.2f}, "
                     f"Final: {final_reward:.2f}")

           # Memory cleanup
           if torch.cuda.is_available():
              torch.cuda.empty_cache()
           gc.collect()

    logger.info(f"Rollout generation completed! Generated {len(all_samples['generated_texts'])} responses")

    # Save rollouts (for fail-safe)
    import pickle
    with open(rollout_cache_file, 'wb') as f:
       pickle.dump(all_samples, f)
    logger.info(f"Saved rollouts to {rollout_cache_file}")

    return all_samples

# ========================================
# 8. PPO Training Components
# ========================================

def compute_advantages_and_returns(rewards, values, gamma=GAMMA, lam=GAE_LAMBDA):
  """
  Compute advantages using Generalized Advantage Estimation (GAE)
  This helps reduce variance in policy gradient estimates
  """
  advantages = []
  returns = []

  for reward_sequence, value_sequence in zip(rewards, values):
      sequence_length = len(reward_sequence)

      # Calculate advantages with GAE
      gae_advantages = np.zeros(sequence_length)
      last_gae = 0

      for t in reversed(range(sequence_length)):
          if t == sequence_length - 1:
              next_value = 0  # Terminal state
          else:
              next_value = value_sequence[t + 1]

          # TD error
          delta = reward_sequence[t] + gamma * next_value - value_sequence[t]
          # GAE formula
          last_gae = delta + gamma * lam * last_gae
          gae_advantages[t] = last_gae

      # Returns are advantages + values
      sequence_returns = gae_advantages + np.array(value_sequence)

      advantages.append(gae_advantages.tolist())
      returns.append(sequence_returns.tolist())

  return advantages, returns

def ppo_update_with_logging(
  policy_model,
  reference_model,
  value_head,
  rollouts,
  optimizer,
  tokenizer,
  iteration_num
):
  """
  Perform PPO update with detailed logging of our scientific explanation metrics
  """
  logger.info(f"Starting PPO update for iteration {iteration_num}...")

  # Extract rollout data
  all_prompt_ids = rollouts["prompt_ids"]
  all_response_ids = rollouts["response_ids"]
  all_logprobs = rollouts["logprobs"]
  all_ref_logprobs = rollouts["ref_logprobs"]
  all_values = rollouts["values"]
  all_rewards = rollouts["rewards"]
  all_questions = rollouts["questions"]
  all_detailed_rewards = rollouts["detailed_rewards"]

  # Log average rewards by tier for monitoring
  avg_tier1 = np.mean([r['tier_scores']['tier1'] for r in all_detailed_rewards])
  avg_tier2 = np.mean([r['tier_scores']['tier2'] for r in all_detailed_rewards])
  avg_tier3 = np.mean([r['tier_scores']['tier3'] for r in all_detailed_rewards])
  avg_final = np.mean([r['final_reward'] for r in all_detailed_rewards])

  logger.info(f"Average rewards - Tier1: {avg_tier1:.2f}, Tier2: {avg_tier2:.2f}, "
              f"Tier3: {avg_tier3:.2f}, Final: {avg_final:.2f}")

  # Compute advantages and returns
  advantages, returns = compute_advantages_and_returns(all_rewards, all_values)

  # Prepare training data
  training_data = []
  for i in range(len(all_response_ids)):
      prompt_ids = all_prompt_ids[i]
      response_ids = all_response_ids[i][:-1]  # Exclude last token
      logprobs = all_logprobs[i]
      seq_advantages = advantages[i]
      seq_returns = returns[i]

      # Ensure all sequences have the same length
      min_len = min(len(response_ids), len(logprobs), len(seq_advantages), len(seq_returns))
      if min_len == 0:
          continue

      response_ids = response_ids[:min_len]
      logprobs = logprobs[:min_len]
      seq_advantages = seq_advantages[:min_len]
      seq_returns = seq_returns[:min_len]

      training_data.append({
          "prompt_ids": prompt_ids,
          "response_ids": response_ids,
          "old_logprobs": torch.tensor(logprobs),
          "advantages": torch.tensor(seq_advantages),
          "returns": torch.tensor(seq_returns),
          "question": all_questions[i],
          "detailed_reward": all_detailed_rewards[i]
      })

  if len(training_data) == 0:
      logger.warning("No valid training examples!")
      return {"policy_loss": 0, "value_loss": 0, "entropy": 0, "kl_div": 0, "clip_frac": 0}

  # Set models to appropriate modes
  policy_model.train()
  value_head.train()
  reference_model.eval()

  # Track metrics for each epoch
  all_metrics = []

  for epoch in range(NUM_EPOCHS):
      logger.info(f"PPO Epoch {epoch+1}/{NUM_EPOCHS}")

      # Shuffle for each epoch
      np.random.shuffle(training_data)

      epoch_metrics = {
          "policy_loss": 0,
          "value_loss": 0,
          "entropy": 0,
          "kl_div": 0,
          "clip_frac": 0,
          "avg_advantage": 0,
          "examples_processed": 0
      }

      for example_idx, example in enumerate(tqdm(training_data, desc=f"Epoch {epoch+1}")):
          prompt_ids = example["prompt_ids"]
          response_ids = example["response_ids"]
          old_logprobs = example["old_logprobs"].to(policy_model.device)
          advantages = example["advantages"].to(policy_model.device)
          returns = example["returns"].to(policy_model.device)

          # Normalize advantages for stable training
          if len(advantages) > 1:
              advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

          # Create full sequence
          full_ids = torch.cat([prompt_ids, response_ids]).unsqueeze(0).to(policy_model.device)
          prompt_length = len(prompt_ids)

          # Forward pass through policy model
          policy_outputs = policy_model(full_ids, output_hidden_states=True)
          policy_logits = policy_outputs.logits[0, prompt_length:-1]

          # Ensure dimensions match
          target_tokens = response_ids.to(policy_model.device)
          if len(target_tokens) > len(policy_logits):
              target_tokens = target_tokens[:len(policy_logits)]
          elif len(target_tokens) < len(policy_logits):
              policy_logits = policy_logits[:len(target_tokens)]

          # Calculate new log probabilities
          new_log_probs = F.log_softmax(policy_logits, dim=-1)
          new_logprobs = new_log_probs.gather(1, target_tokens.unsqueeze(-1)).squeeze(-1)

          # PPO ratio
          ratios = torch.exp(new_logprobs - old_logprobs[:len(new_logprobs)])

          # Clipped surrogate loss
          policy_loss_1 = -advantages[:len(ratios)] * ratios
          policy_loss_2 = -advantages[:len(ratios)] * torch.clamp(ratios, 1.0 - CLIP_EPSILON, 1.0 + CLIP_EPSILON)
          policy_loss = torch.max(policy_loss_1, policy_loss_2).mean()

          # Value function loss
          hidden_states = policy_outputs.hidden_states[-1][0, prompt_length:-1]
          if len(hidden_states) > len(returns):
              hidden_states = hidden_states[:len(returns)]
          elif len(hidden_states) < len(returns):
              returns = returns[:len(hidden_states)]

          values = value_head(hidden_states).squeeze(-1)
          value_loss = 0.5 * ((values - returns) ** 2).mean()

          # Entropy bonus for exploration
          probs = F.softmax(policy_logits, dim=-1)
          entropy = -(probs * new_log_probs).sum(dim=-1).mean()

          # KL divergence penalty to prevent model from changing too much
          with torch.no_grad():
              ref_outputs = reference_model(full_ids)
              ref_logits = ref_outputs.logits[0, prompt_length:-1]
              if len(ref_logits) > len(policy_logits):
                  ref_logits = ref_logits[:len(policy_logits)]
              ref_log_probs = F.log_softmax(ref_logits, dim=-1)

          kl_div = (probs * (new_log_probs - ref_log_probs)).sum(dim=-1).mean()

          # Total loss
          total_loss = (
              policy_loss +
              VALUE_COEF * value_loss -
              ENTROPY_COEF * entropy +
              KL_PENALTY * kl_div
          ) / GRADIENT_ACCUMULATION_STEPS

          # Backward pass
          total_loss.backward()

          # Update metrics
          with torch.no_grad():
              clip_frac = (torch.abs(ratios - 1.0) > CLIP_EPSILON).float().mean()

              epoch_metrics["policy_loss"] += policy_loss.item()
              epoch_metrics["value_loss"] += value_loss.item()
              epoch_metrics["entropy"] += entropy.item()
              epoch_metrics["kl_div"] += kl_div.item()
              epoch_metrics["clip_frac"] += clip_frac.item()
              epoch_metrics["avg_advantage"] += advantages.mean().item()
              epoch_metrics["examples_processed"] += 1

          # Gradient accumulation step
          if (example_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0 or example_idx == len(training_data) - 1:
              # Clip gradients for stability
              torch.nn.utils.clip_grad_norm_(
                  list(policy_model.parameters()) + list(value_head.parameters()),
                  max_norm=MAX_GRAD_NORM
              )
              optimizer.step()
              optimizer.zero_grad()

          # Memory cleanup
          del policy_outputs, ref_outputs
          if torch.cuda.is_available():
              torch.cuda.empty_cache()

      # Average epoch metrics
      if epoch_metrics["examples_processed"] > 0:
          for key in epoch_metrics:
              if key != "examples_processed":
                  epoch_metrics[key] /= epoch_metrics["examples_processed"]

      all_metrics.append(epoch_metrics)
      logger.info(f"Epoch {epoch+1} metrics: {epoch_metrics}")

  # Calculate final averaged metrics
  final_metrics = {}
  if all_metrics:
      for key in all_metrics[0].keys():
          if key != "examples_processed":
              final_metrics[key] = np.mean([m[key] for m in all_metrics])

  logger.info(f"PPO update completed! Final metrics: {final_metrics}")

  return final_metrics

# ========================================
# 9. Main Training Loop with Progress Plotting
# ========================================

def plot_training_progress_live(training_history):
   """Plot training progress during training (simplified for live updates)"""
   plt.figure(figsize=(12, 8))

   iterations = training_history['iterations']

   # Subplot 1: Overall Reward Progress
   plt.subplot(2, 2, 1)
   avg_rewards = [np.mean(r) for r in training_history['rewards']]
   plt.plot(iterations, avg_rewards, 'b-', linewidth=2, marker='o')
   plt.title('Overall Reward Progress')
   plt.xlabel('Iteration')
   plt.ylabel('Average Reward (0-10)')
   plt.grid(True, alpha=0.3)

   # Subplot 2: Tier-wise Rewards
   plt.subplot(2, 2, 2)
   plt.plot(iterations, training_history['tier1_rewards'], 'r-', linewidth=2, marker='o', label='Tier 1')
   plt.plot(iterations, training_history['tier2_rewards'], 'g-', linewidth=2, marker='o', label='Tier 2')
   plt.plot(iterations, training_history['tier3_rewards'], 'b-', linewidth=2, marker='o', label='Tier 3')
   plt.title('Rewards by Tier')
   plt.xlabel('Iteration')
   plt.ylabel('Average Score (0-10)')
   plt.legend()
   plt.grid(True, alpha=0.3)

   # Subplot 3: Training Losses
   plt.subplot(2, 2, 3)
   policy_losses = [m['policy_loss'] for m in training_history['metrics']]
   value_losses = [m['value_loss'] for m in training_history['metrics']]
   plt.plot(iterations, policy_losses, 'purple', linewidth=2, marker='o', label='Policy Loss')
   plt.plot(iterations, value_losses, 'orange', linewidth=2, marker='o', label='Value Loss')
   plt.title('Training Losses')
   plt.xlabel('Iteration')
   plt.ylabel('Loss')
   plt.legend()
   plt.grid(True, alpha=0.3)

   # Subplot 4: KL Divergence
   plt.subplot(2, 2, 4)
   kl_divs = [m['kl_div'] for m in training_history['metrics']]
   plt.plot(iterations, kl_divs, 'green', linewidth=2, marker='o')
   plt.title('KL Divergence')
   plt.xlabel('Iteration')
   plt.ylabel('KL Div')
   plt.grid(True, alpha=0.3)

   plt.tight_layout()
   plt.savefig(f'{ADAPTER_PATH}/training_progress_iter_{iterations[-1]}.png', dpi=150)
   plt.show()
   plt.close()

def train_with_deepseek_rewards(questions, num_iterations=3, force_restart=False):
  """
  Main training function that orchestrates the entire PPO + DeepSeek reward process
  With option to force restart training from scratch
  """
  logger.info("="*80)
  logger.info("Starting SciGuru PPO Training with DeepSeek Reward Model")
  logger.info(f"Training on {len(questions)} questions for {num_iterations} iterations")
  logger.info(f"Force restart: {force_restart}")
  logger.info("="*80)

  # Handle cleanup if force_restart is True
  if force_restart:
      logger.info("Force restart requested - cleaning up previous artifacts...")

      # Remove old checkpoints
      for checkpoint in glob.glob(f"{ADAPTER_PATH}/checkpoint-*"):
          shutil.rmtree(checkpoint)
          logger.info(f"Removed {checkpoint}")

      # Remove old rollout caches
      for cache in glob.glob(f"{ADAPTER_PATH}/rollouts_iter_*.pkl"):
          os.remove(cache)
          logger.info(f"Removed {cache}")

      # Clear reward cache
      if os.path.exists(REWARD_CACHE_PATH):
          os.remove(REWARD_CACHE_PATH)
          logger.info("Cleared reward cache")

      # Remove any existing adapter files
      for file in ['adapter_config.json', 'adapter_model.bin', 'value_head.pt']:
          filepath = f"{ADAPTER_PATH}/{file}"
          if os.path.exists(filepath):
              os.remove(filepath)
              logger.info(f"Removed {filepath}")

  # Initialize reward model
  reward_model = DeepSeekRewardModel(DEEPSEEK_API_KEY)
  logger.info("DeepSeek reward model initialized")
  logger.info(f"Calibration stats: {reward_model.get_calibration_stats()}")

  # Load models
  policy_model, reference_model, value_head, tokenizer = load_models()

  # Set up optimizer for trainable parameters
  trainable_params = list(policy_model.parameters()) + list(value_head.parameters())
  optimizer = torch.optim.Adam(trainable_params, lr=LEARNING_RATE, eps=1e-5)

  # Track training metrics
  training_history = {
     "iterations": [],
     "metrics": [],
     "rewards": [],
     "tier1_rewards": [],
     "tier2_rewards": [],
     "tier3_rewards": [],
     "individual_metric_rewards": defaultdict(list)
 }

  # Determine start iteration
  start_iteration = 0
  if not force_restart:
      start_iteration = get_resume_iteration(ADAPTER_PATH)
      if start_iteration > 0:
          logger.info(f"Found existing checkpoints up to iteration {start_iteration}")
          logger.info(f"Resuming from iteration {start_iteration}")

          # Try to load previous training history
          history_file = f"{ADAPTER_PATH}/checkpoint-{start_iteration}/training_history.json"
          if os.path.exists(history_file):
              with open(history_file, 'r') as f:
                  training_history = json.load(f)
              logger.info("Loaded previous training history")

  # Main training loop
  for iteration in range(start_iteration, num_iterations):
      logger.info(f"\n{'='*60}")
      logger.info(f"Starting PPO Iteration {iteration+1}/{num_iterations}")
      logger.info(f"{'='*60}")

      log_memory_usage(f"Before iteration {iteration+1}")

      # 1. Generate rollouts with DeepSeek rewards
      rollouts = generate_rollouts_with_rewards(
          policy_model=policy_model,
          reference_model=reference_model,
          value_head=value_head,
          tokenizer=tokenizer,
          prompts=questions,
          reward_model=reward_model,
          iteration=iteration
      )

      # Check if we have valid rollouts
      if len(rollouts["response_ids"]) == 0:
          logger.warning("No valid rollouts generated, skipping iteration")
          continue

      # 2. Perform PPO update
      metrics = ppo_update_with_logging(
          policy_model=policy_model,
          reference_model=reference_model,
          value_head=value_head,
          rollouts=rollouts,
          optimizer=optimizer,
          tokenizer=tokenizer,
          iteration_num=iteration+1
      )

      # 3. Log progress
      logger.info(f"\nIteration {iteration+1} Summary:")
      logger.info(f"Policy Loss: {metrics['policy_loss']:.4f}")
      logger.info(f"Value Loss: {metrics['value_loss']:.4f}")
      logger.info(f"KL Divergence: {metrics['kl_div']:.4f}")
      logger.info(f"Entropy: {metrics['entropy']:.4f}")

      # Store metrics
      training_history["iterations"].append(iteration + 1)
      training_history["metrics"].append(metrics)
      training_history["rewards"].append([r['final_reward'] for r in rollouts['detailed_rewards']])

      # Extract detailed reward information for tracking
      tier1_rewards = [r['tier_scores']['tier1'] for r in rollouts['detailed_rewards']]
      tier2_rewards = [r['tier_scores']['tier2'] for r in rollouts['detailed_rewards']]
      tier3_rewards = [r['tier_scores']['tier3'] for r in rollouts['detailed_rewards']]

      # Track individual metrics
      for reward_detail in rollouts['detailed_rewards']:
          for metric, score in reward_detail['individual_scores'].items():
              training_history["individual_metric_rewards"][metric].append(score)

      # Store in history
      training_history["tier1_rewards"].append(np.mean(tier1_rewards))
      training_history["tier2_rewards"].append(np.mean(tier2_rewards))
      training_history["tier3_rewards"].append(np.mean(tier3_rewards))

      # 4. Plot training progress live
      logger.info("Plotting training progress...")
      plot_training_progress_live(training_history)

      # 5. Save checkpoint
      checkpoint_path = f"{ADAPTER_PATH}/checkpoint-{iteration+1}"
      os.makedirs(checkpoint_path, exist_ok=True)

      logger.info(f"Saving checkpoint to {checkpoint_path}")
      policy_model.save_pretrained(checkpoint_path)
      torch.save(value_head.state_dict(), f"{checkpoint_path}/value_head.pt")

      # Save training history
      with open(f"{checkpoint_path}/training_history.json", 'w') as f:
          json.dump(training_history, f, indent=2)

      # 6. Sample generation for quality check
      if (iteration + 1) % 2 == 0:  # Every 2 iterations
          logger.info("\nGenerating sample explanations for quality check...")
          sample_questions = questions[:3]  # First 3 questions
          for q in sample_questions:
              response = generate_text(policy_model, tokenizer, q)
              logger.info(f"\nQ: {q}")
              logger.info(f"A: {response[:200]}...")

      # 7. Clear memory
      del rollouts
      if torch.cuda.is_available():
          torch.cuda.empty_cache()
      gc.collect()

      log_memory_usage(f"After iteration {iteration+1}")

  # Save final model
  logger.info("\nSaving final model...")
  policy_model.save_pretrained(ADAPTER_PATH)
  torch.save(value_head.state_dict(), f"{ADAPTER_PATH}/value_head.pt")

  # Save complete training history
  with open(f"{ADAPTER_PATH}/complete_training_history.json", 'w') as f:
      json.dump(training_history, f, indent=2)

  logger.info("\nTraining completed successfully!")

  return policy_model, value_head, tokenizer, training_history

# ========================================
# 10. Evaluation Functions
# ========================================

def generate_text(model, tokenizer, prompt, max_length=256):
  """Generate text for evaluation"""
  model.eval()

  formatted_prompt = format_prompt(prompt)
  inputs = tokenizer(formatted_prompt, return_tensors="pt", truncation=True).to(model.device)

  with torch.no_grad():
      outputs = model.generate(
          inputs.input_ids,
          attention_mask=inputs.attention_mask,
          max_new_tokens=max_length,
          do_sample=True,
          temperature=0.7,  # Balanced creativity
          top_p=0.9,
          pad_token_id=tokenizer.pad_token_id
      )

  prompt_length = inputs.input_ids.shape[1]
  generated_text = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True)

  return generated_text

def evaluate_model_comprehensive(model, tokenizer, test_questions, reward_model):
  """
  Comprehensive evaluation showing improvement in explanation quality
  """
  logger.info("\n" + "="*80)
  logger.info("COMPREHENSIVE MODEL EVALUATION")
  logger.info("="*80)

  results = []
  total_scores = defaultdict(float)

  for i, question in enumerate(test_questions):
      logger.info(f"\nEvaluating question {i+1}/{len(test_questions)}")
      logger.info(f"Question: {question}")

      # Generate response
      response = generate_text(model, tokenizer, question)
      logger.info(f"Response: {response[:200]}...")

      # Get detailed evaluation
      final_reward, detailed_scores = reward_model.calculate_reward(question, response)

      # Log scores
      logger.info(f"\nScores:")
      logger.info(f"Final Reward: {final_reward:.2f}")
      logger.info(f"Tier 1 (Pedagogy): {detailed_scores['tier_scores']['tier1']:.2f}")
      logger.info(f"Tier 2 (Tools): {detailed_scores['tier_scores']['tier2']:.2f}")
      logger.info(f"Tier 3 (Secondary): {detailed_scores['tier_scores']['tier3']:.2f}")

      # Individual metrics
      logger.info("\nIndividual Metrics:")
      for metric, score in detailed_scores['individual_scores'].items():
          logger.info(f"  {metric}: {score:.2f}")
          total_scores[metric] += score

      results.append({
          'question': question,
          'response': response,
          'scores': detailed_scores
      })

      logger.info("-" * 60)

  # Calculate averages
  logger.info("\n" + "="*60)
  logger.info("AVERAGE SCORES ACROSS ALL QUESTIONS:")
  logger.info("="*60)

  num_questions = len(test_questions)
  for metric, total in total_scores.items():
      avg = total / num_questions
      logger.info(f"{metric}: {avg:.2f}")

  return results


def save_reward_progression(training_history, filepath="reward_progression.csv"):
   """Save reward progression to CSV for external analysis"""
   import pandas as pd

   data = {
       'iteration': training_history['iterations'],
       'avg_final_reward': [np.mean(r) for r in training_history['rewards']],
       'tier1_avg': training_history['tier1_rewards'],
       'tier2_avg': training_history['tier2_rewards'],
       'tier3_avg': training_history['tier3_rewards']
   }

   # Add individual metrics
   for metric, scores in training_history['individual_metric_rewards'].items():
       # Ensure same length
       metric_scores = scores[:len(training_history['iterations'])]
       while len(metric_scores) < len(training_history['iterations']):
           metric_scores.append(metric_scores[-1] if metric_scores else 5.0)
       data[f'metric_{metric}'] = metric_scores

   df = pd.DataFrame(data)
   df.to_csv(filepath, index=False)
   logger.info(f"Reward progression saved to {filepath}")

# ========================================
# 11. Visualization Functions
# ========================================

def plot_training_progress(training_history):
   """Visualize training progress with detailed reward tracking"""
   iterations = training_history['iterations']

   # Extract metrics
   policy_losses = [m['policy_loss'] for m in training_history['metrics']]
   value_losses = [m['value_loss'] for m in training_history['metrics']]
   kl_divs = [m['kl_div'] for m in training_history['metrics']]
   avg_rewards = [np.mean(r) for r in training_history['rewards']]

   # Create figure with more subplots
   fig = plt.figure(figsize=(16, 12))

   # Main title
   fig.suptitle('SciGuru Training Progress - Comprehensive Analysis', fontsize=16)

   # 1. Overall Reward Progress (Large subplot at top)
   ax1 = plt.subplot(3, 3, (1, 3))
   ax1.plot(iterations, avg_rewards, 'b-', linewidth=3, label='Final Reward')
   ax1.fill_between(iterations, avg_rewards, alpha=0.3)
   ax1.set_title('Overall Reward Progress', fontsize=14)
   ax1.set_xlabel('Iteration')
   ax1.set_ylabel('Average Reward (0-10)')
   ax1.grid(True, alpha=0.3)
   ax1.legend()

   # 2. Tier-wise Rewards
   ax2 = plt.subplot(3, 3, 4)
   ax2.plot(iterations, training_history['tier1_rewards'], 'r-', linewidth=2, label='Tier 1 (Pedagogy)')
   ax2.plot(iterations, training_history['tier2_rewards'], 'g-', linewidth=2, label='Tier 2 (Tools)')
   ax2.plot(iterations, training_history['tier3_rewards'], 'b-', linewidth=2, label='Tier 3 (Secondary)')
   ax2.set_title('Rewards by Tier')
   ax2.set_xlabel('Iteration')
   ax2.set_ylabel('Average Score (0-10)')
   ax2.grid(True, alpha=0.3)
   ax2.legend()

   # 3. Policy Loss
   ax3 = plt.subplot(3, 3, 5)
   ax3.plot(iterations, policy_losses, 'purple', linewidth=2)
   ax3.set_title('Policy Loss')
   ax3.set_xlabel('Iteration')
   ax3.set_ylabel('Loss')
   ax3.grid(True, alpha=0.3)

   # 4. Value Loss
   ax4 = plt.subplot(3, 3, 6)
   ax4.plot(iterations, value_losses, 'orange', linewidth=2)
   ax4.set_title('Value Loss')
   ax4.set_xlabel('Iteration')
   ax4.set_ylabel('Loss')
   ax4.grid(True, alpha=0.3)

   # 5. Individual Metrics Heatmap
   ax5 = plt.subplot(3, 3, (7, 9))

   # Prepare data for heatmap
   metrics_names = list(training_history['individual_metric_rewards'].keys())
   metrics_data = []
   for metric in metrics_names:
       # Pad with last value if needed
       metric_scores = training_history['individual_metric_rewards'][metric]
       while len(metric_scores) < len(iterations):
           metric_scores.append(metric_scores[-1] if metric_scores else 5.0)
       metrics_data.append(metric_scores[:len(iterations)])

   if metrics_data:
       im = ax5.imshow(metrics_data, aspect='auto', cmap='RdYlGn', vmin=0, vmax=10)
       ax5.set_yticks(range(len(metrics_names)))
       ax5.set_yticklabels(metrics_names)
       ax5.set_xticks(range(len(iterations)))
       ax5.set_xticklabels(iterations)
       ax5.set_xlabel('Iteration')
       ax5.set_title('Individual Metric Scores Over Time')

       # Add colorbar
       cbar = plt.colorbar(im, ax=ax5)
       cbar.set_label('Score (0-10)', rotation=270, labelpad=15)

   plt.tight_layout()
   plt.savefig('training_progress_detailed.png', dpi=150, bbox_inches='tight')
   plt.show()

   # Additional plot for reward improvement
   fig2, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

   # Reward improvement percentage
   if len(avg_rewards) > 1:
       improvement = [(r - avg_rewards[0]) / avg_rewards[0] * 100 for r in avg_rewards]
       ax1.bar(iterations, improvement, color='green', alpha=0.7)
       ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
       ax1.set_title('Reward Improvement from Baseline (%)')
       ax1.set_xlabel('Iteration')
       ax1.set_ylabel('Improvement %')
       ax1.grid(True, alpha=0.3)

   # Box plot of reward distribution
   ax2.boxplot(training_history['rewards'], positions=iterations)
   ax2.set_title('Reward Distribution per Iteration')
   ax2.set_xlabel('Iteration')
   ax2.set_ylabel('Reward (0-10)')
   ax2.grid(True, alpha=0.3)

   plt.tight_layout()
   plt.savefig('reward_analysis.png', dpi=150, bbox_inches='tight')
   plt.show()

# ========================================
# 12. Main Execution
# ========================================

def main():
  """
  Main function to run the complete training pipeline
  """
  logger.info("\n" + "="*80)
  logger.info("SCIGURU - Scientific Explanation Enhancement via PPO + DeepSeek")
  logger.info("="*80)

  # Get training questions
  all_questions = getQuestions()
  questions =  all_questions[235:265]  # 30 questions for training
  logger.info(f"Loaded {len(questions)} training questions")

  # IMPORTANT: Set this to True if you want to start fresh training
  FORCE_RESTART = False  # Change to False if you want to resume from checkpoints

  # Run training
  trained_model, value_head, tokenizer, training_history = train_with_deepseek_rewards(
      questions=questions,
      num_iterations=3,  # Start with 3 iterations for testing
      force_restart=FORCE_RESTART
  )

  # Visualize progress
  logger.info("\nGenerating training progress plots...")
  plot_training_progress(training_history)

  # Save reward progression data
  save_reward_progression(training_history, f"{ADAPTER_PATH}/reward_progression.csv")

  # Evaluate on test set
  test_questions = all_questions[51:56]  # 5 questions for testing

  # Initialize reward model for evaluation
  reward_model = DeepSeekRewardModel(DEEPSEEK_API_KEY)

  logger.info("\nRunning comprehensive evaluation...")
  evaluation_results = evaluate_model_comprehensive(
      trained_model,
      tokenizer,
      test_questions,
      reward_model
  )

  # Save evaluation results
  with open(f"{ADAPTER_PATH}/evaluation_results.json", 'w') as f:
      json.dump(evaluation_results, f, indent=2)

  logger.info("\n" + "="*80)
  logger.info("TRAINING AND EVALUATION COMPLETE!")
  logger.info(f"Model saved to: {ADAPTER_PATH}")
  logger.info(f"Evaluation saved to: {ADAPTER_PATH}/evaluation_results.json")
  logger.info("="*80)

# Run the main function
if __name__ == "__main__":
  main()