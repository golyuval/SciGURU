{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT API Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç© https://wandb.ai/golyuvalg-ben-gurion-university-of-the-negev/intro-example/r/call/0194e789-1011-7053-92ce-2d2a20200f16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'verb count': 3, 'adj count': 4, 'total word count': 26}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weave\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "@weave.op() # üêù Decorator to track requests\n",
    "def extract_fruit(sentence: str) -> dict:\n",
    "    \n",
    "    client = OpenAI(api_key=openai_key)\n",
    "    system_prompt = \"Parse sentences into a JSON dict with keys: verb count, adj count and total word cound.\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": sentence}\n",
    "      ],\n",
    "      temperature=0.7,\n",
    "      response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "\n",
    "    extracted = response.choices[0].message.content\n",
    "\n",
    "    return json.loads(extracted)\n",
    "\n",
    "weave.init('intro-example') # üêù\n",
    "\n",
    "sentence = \"There are many fruits that were found on the recently discovered planet Goocrux. There are neoskizzles that grow there, which are purple and taste like candy.\"\n",
    "\n",
    "extract_fruit(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LLM** : from the inside"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Module Types\n",
    "\n",
    "    - Parameters\n",
    "    - Gradients\n",
    "    - Optimizer States\n",
    "    - Activations\n",
    "\n",
    "- ### Layer Types\n",
    "\n",
    "    - Convolutional Layer\n",
    "    - Subsampling Layer\n",
    "    - Functional Layer\n",
    "    - Fully connected Layer\n",
    "    - Gaussian Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PEFT** : Parameter Efficient Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "- ### **PEFT Types**\n",
    "\n",
    "    - **Selective** - subset of layers\n",
    "\n",
    "    - **Additive** - add layers on top\n",
    "\n",
    "    - **Reparameterization** - add layers in between to simplify the computation\n",
    "\n",
    "        - LoRA - only add the layers to reduce computations\n",
    "\n",
    "        - QLoRA - LoRA + quantization technics\n",
    "\n",
    "        - DoRA - Adjusts the rank of the low-rank space based on the magnitude of the components\n",
    "\n",
    "</br>\n",
    "\n",
    "- ### **Quantization Types**\n",
    "\n",
    "    | Quantization Type                 | Explaination                                                                                  | \n",
    "    |-----------------------------------|-----------------------------------------------------------------------------------------------|\n",
    "    | NF4                               | normalized 4-bit floating point quantization                                                  |\n",
    "    | FP16 (half precision)             | 16-bit floating point quantization (cut memory usage in half)                                 |\n",
    "    | BF16 (BFloat16)                   | 16-bit floating point (larger exponent range - better numerical stability)                    |\n",
    "    | Int8                              | often applied in post-training (sometimes with calibration steps)                             |\n",
    "    | 4/8-bit quantization              | only for extremely large models (dangerous for maintaining accuracy)                          |\n",
    "    |                                   |                                                                                               |               \n",
    "    | Dynamic Quantization              | quantize weight on the fly during inference                                                   |\n",
    "    | Static Quantization (PTQ)         | quantize after the model is fully trained                                                     |\n",
    "    | Quantization Aware Training (QAT) | generally yielding the best performance when retraining is possible                           |\n",
    "    | Mixed Precision Training          | Using lower precision for most calculations while preserving higher precision where needed.   |\n",
    "\n",
    "</br>\n",
    "\n",
    "- ### **Others**\n",
    "\n",
    "    - Soft Prompts\n",
    "    - LoftQ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PEFT Demo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model_pretrained = AutoModelForSequenceClassification(\"distilbert-base-uncased\")\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = dict((v,k) for k,v in id2label.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform to Quantized Pretraied Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "# quantization configuration\n",
    "\n",
    "quantization_configuration = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                                      # quantize the model to 4-bits when loaded\n",
    "    bnb_4bit_quant_type=\"nf4\",                              # use NF4 data type for weights (initialized from a normal distribution)\n",
    "    bnb_4bit_use_double_quant=True,                         # nested quantization scheme (to double quantize the weights)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,                  # use bfloat16 for faster computation\n",
    "    llm_int8_skip_modules=[\"classifier\", \"pre_classifier\"]  # Don't convert the \"classifier\" and \"pre_classifier\" layers to 8-bit\n",
    ")\n",
    "\n",
    "# NOT ready for quantized training yet ... \n",
    "\n",
    "model_quantized = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",\n",
    "                                                                 id2label=id2label,\n",
    "                                                                 label2id=label2id,\n",
    "                                                                 num_labels=2,\n",
    "                                                                 quantization_config=quantization_configuration\n",
    "                                                                )\n",
    "\n",
    "# Ready for quantized training\n",
    "\n",
    "model_quantized = prepare_model_for_kbit_training(model_quantized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform to QLoRA Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# LoRA configurations\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,                 # Specify the task type as sequence classification\n",
    "    r=8,                                        # Rank of the low-rank matrices\n",
    "    lora_alpha=16,                              # Scaling factor\n",
    "    lora_dropout=0.1,                           # Dropout rate  \n",
    "    target_modules=['q_lin','k_lin','v_lin']    # which modules\n",
    ")\n",
    "\n",
    "# Apply LoRA on the quantized model\n",
    "\n",
    "model_qlora = get_peft_model(model_quantized, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply LoftQ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import replace_lora_weights_loftq\n",
    "\n",
    "replace_lora_weights_loftq(model_qlora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TrainingArguments' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m(\n\u001b[0;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results_qlora\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      4\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m      5\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m      6\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,\n\u001b[0;32m      7\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Mockups\n",
    "tokenized_train, tokenized_test, compute_metrics = None,None,None\n",
    "\n",
    "# training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_qlora\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# QLoRA trainer\n",
    "\n",
    "trainer_qlora = Trainer(\n",
    "    model=model_qlora,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "# Train the model\n",
    "\n",
    "trainer_qlora.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Types\n",
    "\n",
    "- Text Generation\n",
    "\n",
    "- Text Translation\n",
    "\n",
    "- Text Summerization\n",
    "\n",
    "- Text Classification\n",
    "\n",
    "- Question Answering\n",
    "\n",
    "- Zero Shot Classification :  Classify text into categories without training data\n",
    "\n",
    "- Named Entity Recognition (NER) : extracting structured info out of unstructured text\n",
    "\n",
    "- Fill-Mask (???)\n",
    "\n",
    "- Feature Extraction (???)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare For Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# suppress warnings \n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No pipeline - sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Sample text\n",
    "\n",
    "text = \"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# performing inference (grad needed only in training - not inference)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Convert logits to probabilities\n",
    "\n",
    "probs = torch.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "# Get the predicted class\n",
    "\n",
    "predicted_class = torch.argmax(probs, dim=-1)\n",
    "\n",
    "# Map the predicted class to the label\n",
    "\n",
    "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "predicted_label = labels[predicted_class]\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997586607933044}]\n"
     ]
    }
   ],
   "source": [
    "# Load a general text classification model\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Classify a sample text\n",
    "result = classifier(\"Congratulations! You've won a free ticket to the Bahamas. Reply WIN to claim.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - language detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'fr', 'score': 0.9934879541397095}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"papluca/xlm-roberta-base-language-detection\")\n",
    "result = classifier(\"Bonjour, comment √ßa va?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Pipeline - GPT2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Prompt\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "\n",
    "# Tokenize the input text\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate text\n",
    "output_ids = model.generate(\n",
    "    inputs.input_ids, \n",
    "    attention_mask=inputs.attention_mask,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=50, \n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - GPT2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time he looked like a man and a lion, and once upon a time he was such as would have to be put at the head of another human being. He had the blood of a great lion and the blood of a wolf and\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text generation pipeline with GPT-2\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Generate text based on a given prompt\n",
    "prompt = \"Once upon a time\"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1, truncation=True)\n",
    "\n",
    "# Print the generated text\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - T5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment √™tes-vous?\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text generation pipeline with T5\n",
    "generator = pipeline(\"text2text-generation\", model=\"t5-small\")\n",
    "\n",
    "# Generate text based on a given prompt\n",
    "prompt = \"translate English to French: How are you?\"\n",
    "result = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "\n",
    "# Print the generated text\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline - Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.4167883098125458, 'token': 3000, 'token_str': 'paris', 'sequence': 'the capital of france is paris.'}, {'score': 0.07141656428575516, 'token': 22479, 'token_str': 'lille', 'sequence': 'the capital of france is lille.'}, {'score': 0.06339260935783386, 'token': 10241, 'token_str': 'lyon', 'sequence': 'the capital of france is lyon.'}, {'score': 0.04444736987352371, 'token': 16766, 'token_str': 'marseille', 'sequence': 'the capital of france is marseille.'}, {'score': 0.030297178775072098, 'token': 7562, 'token_str': 'tours', 'sequence': 'the capital of france is tours.'}]\n"
     ]
    }
   ],
   "source": [
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "prompt = \"The capital of France is [MASK].\"\n",
    "result = fill_mask(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
